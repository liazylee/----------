{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named scarpy",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-42f6ed5e036f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mscarpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscarpy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mdic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named scarpy"
     ]
    }
   ],
   "source": [
    "#杂七杂八\n",
    "\n",
    "\n",
    "\n",
    "import scarpy\n",
    "dic = dir(scarpy)\n",
    "print dic\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "import urllib\n",
    "import re\n",
    "import random\n",
    "from time import sleep\n",
    "def main():\n",
    "    url = 'https://www.zhihu.com/question/22591304/followersf4e428'\n",
    "    headers = {}\n",
    "    i=1\n",
    "    for x in xrange(20,2600,20):\n",
    "        data = { 'start':'0','offset':str(x),'_xsrf':'a128464ef225a69348cef94c38'}\n",
    "        content=requests.post(url,headers=headers,data=data,timeout=10).text\n",
    "        imgs = re.findall('<img src=\\\\\\\\\\\"(.*?)_m.jpg',content)\n",
    "        for img in imgs:\n",
    "            try:\n",
    "                img=img.replace('\\\\','')\n",
    "                pic = img+'.jpg'\n",
    "                path = 'd:\\\\bs4\\\\zhihu\\\\jpg\\\\str(i)+'/jpg\n",
    "                urllib.urlretrieve(pic,path)\n",
    "                print u'下载了第'+str(i)+u'张图片'\n",
    "                i += 1\n",
    "                sleep(random.uniform(0.5,1))\n",
    "            except:\n",
    "                print u'抓漏一张'\n",
    "                pass\n",
    "                sleep(random.uniform(0.5,1))\n",
    "if __name__=='__main__':\n",
    "    main()\n",
    "                \n",
    "                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'urlretrieve' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-fb0820a66d1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mgetTopicList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-fb0820a66d1e>\u001b[0m in \u001b[0;36mgetTopicList\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34mu'晒'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                     \u001b[0mi_href\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'href'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                     \u001b[0mgetTopicContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_href\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#获取帖子里的内容\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-fb0820a66d1e>\u001b[0m in \u001b[0;36mgetTopicContext\u001b[0;34m(topicUrl)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0msaveImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"src\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m#下载图片到本地\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-fb0820a66d1e>\u001b[0m in \u001b[0;36msaveImage\u001b[0;34m(imgUrl)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mfileName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgUrl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimgUrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr\"d:\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfileName\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgUrl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'urlretrieve' is not defined"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import requests,re,json,sys,time\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "\n",
    "url=\"https://www.douban.com/group/haixiuzu/\"\n",
    "\n",
    "def getTopicList():\n",
    "    #循环分页抓取 括号里的换成需要抓取的页数区间，从0开始，间隔越大，时间越长\n",
    "    for x in range(16,20):\n",
    "        page = x * 25\n",
    "        get_url = requests.get(url+\"discussion?start=\"+str(page))\n",
    "       \n",
    "        soup = BeautifulSoup(get_url.text,\"html.parser\")\n",
    "        tdList = soup.find_all(\"td\",class_='title')       \n",
    "        for i in tdList:\n",
    "            title = i.a.get(\"title\")\n",
    "            if len(i.contents) > 1:\n",
    "                #因为大家晒照片都喜欢在前面加一个【晒】\n",
    "                if u'晒' in title:\n",
    "                    i_href = i.a.get('href')\n",
    "                    getTopicContext(i_href)\n",
    "        time.sleep(1)\n",
    "#获取帖子里的内容\n",
    "def getTopicContext(topicUrl):\n",
    "    url = requests.get(topicUrl)\n",
    "    soup = BeautifulSoup(url.text,\"html.parser\")\n",
    "    topicDiv = soup.find_all(\"div\",class_='topic-figure cc')\n",
    "    for div in topicDiv:\n",
    "        if len(div.contents) > 1:\n",
    "            img = div.img\n",
    "            saveImage(img.get(\"src\"))\n",
    "\n",
    "#下载图片到本地\n",
    "def saveImage(imgUrl):\n",
    "    fileName = imgUrl[imgUrl.rfind(\"/\")+1:]\n",
    "    path = r\"d:\"+fileName \n",
    "    urlretrieve(imgUrl,path)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    getTopicList()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scrapy.item import Item, field\n",
    "class TutorialItem(Item):\n",
    "    #defind teh fields for youy item here like:\n",
    "    #name = field()\n",
    "    #电影名字 \n",
    "    movie_name = field()\n",
    "    movie_director = field()\n",
    "    movie_writer =field()\n",
    "    movie_roles = field()\n",
    "    movie_language = field()\n",
    "    movie_date = field()\n",
    "    movie_long =field()\n",
    "    movie_description =field()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coding = utf-8\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "from scrapy.spider import BaseSpider\n",
    "from scrapy.http import Request\n",
    "from scrapy.selector import HtmlXPathSelector\n",
    "from tutorial.items import TutorialItem\n",
    "import re\n",
    "class DoubanSpider(BaseSpider):\n",
    "    name = 'douban'\n",
    "    allowed_domains = ['movie.douban.com']\n",
    "    start_urls = []\n",
    "    def start_requests(self):\n",
    "        file_object = open('movie_name.txt','r')\n",
    "        try:\n",
    "            url_head = 'http://movie.douban.com/subject_search?search_text='\n",
    "            for line in file_object:\n",
    "                self.start_urls.append(url_head+line)\n",
    "            for url in self.start_urls:\n",
    "                yield self.make_requests_from_url(url)\n",
    "        finally:\n",
    "            file_object.close()\n",
    "    def parse(self,response):\n",
    "        #open('test.html','wb')\n",
    "            \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named tutorial.items",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9d28e2750241>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscrapy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscrapy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselector\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHtmlXPathSelector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtutorial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTutorialItem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDoubanSpaider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseSpider\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named tutorial.items"
     ]
    }
   ],
   "source": [
    "#coding = utf-8\n",
    "import sys\n",
    "reload(sys)\n",
    "#python默认环境编码是ascii\n",
    "sys.setdefaultencoding('utf-8')\n",
    "from scrapy.spider import BaseSpider\n",
    "from scrapy.http import Request\n",
    "from scrapy.selector import HtmlXPathSelector\n",
    "from tutorial.items import TutorialItem\n",
    "import re\n",
    "class DoubanSpaider(BaseSpider):\n",
    "    name = 'douban'\n",
    "    allowed_domaina = ['movie.douban.com']\n",
    "    start_urls = []\n",
    "    def start_requesta(self):\n",
    "        file_object = open('movie_name.txt','r')\n",
    "        try:\n",
    "            url_head = 'http://movie.douban.com/subject_search?search_text='\n",
    "            for line in file_object:\n",
    "                self.start_urls.append(url_head + line)\n",
    "            for url in self.start_urls:\n",
    "                yield self.make_requests_from_url(url)\n",
    "                #这个用法也没定义过 ，可能是库里面的\n",
    "        finally:\n",
    "            file_object.close()\n",
    "    def parse(self,response):\n",
    "        hxs = HtmlXPathSelector(response)\n",
    "        #movie_name = hxs.select('//*[@id=\"content\"]/div/div[1]/div[2]/table[1]/tr/td[1]/a/@title').extract()\n",
    "        movie_link = hxs.select('//*[@id=\"content\"]/div/div[1]/div[2]/table[1]/tr/td[1]/a/@href').extract()\n",
    "        #这个地方id是什么  估计是个html里面的\n",
    "        if movie_link:\n",
    "            yield Request(movie_link[0],callback = self.parse_item)\n",
    "            #下面定义了parse_item\n",
    "    def parse_item(self,response):\n",
    "        hxs = HtmlXPathSelector(response)\n",
    "        movie_name = hxs.select('//*[@id=\"content\"]/h1/span[1]/text()').extract()\n",
    "        movie_director = hxs.select('//*[@id=\"info\"]/span[1]/span[2]/a/text()').extract()\n",
    "        movie_writer = hxs.select('//*[@id=\"info\"]/span[2]/span[2]/a/text()').extract()\n",
    "        #爬取电影详情需要在已有对象中继续爬取\n",
    "        movie_description_paths = hxs.select('//*[@id=\"link-report\"]')\n",
    "        movie_description = []\n",
    "        for movie_description_path in movie_description_paths:\n",
    "            movie_description = movie_description_path.select('.//*[@property=\"v:summary\"]/text()').extract()\n",
    "            #后面的又看不懂了 \n",
    "        movie_roles_paths = hxs.select('//*[@id=\"info\"]/span[3]/span[2]')\n",
    "        movie_roles = []\n",
    "        for movie_roles_path in movie_roles_paths:\n",
    "            movie_roles = movie_roles_path.select('.//*[@rel=\"v:starring\"]/text()').extract()\n",
    "        movie_detail = hxs.select('//*[@id=\"info\"]').extract()\n",
    "        item = TutorialItem()\n",
    "        item['movie_name'] = ''.join(movie_name).strip().replace(',',';').replace('\\'','\\\\\\'').replace('\\\"','\\\\\\\"').replace(':',';')\n",
    "        #item['movie_link'] = movie_link[0]\n",
    "        item['movie_director'] = movie_director[0].strip().replace(',',';').replace('\\'','\\\\\\'').replace('\\\"','\\\\\\\"').replace(':',';') if len(movie_director) > 0 else ''\n",
    "        item['movie_description'] = movie_description[0].strip().replace(',',';').replace('\\'','\\\\\\'').replace('\\\"','\\\\\\\"').replace(':',';') if len(movie_description) > 0 else ''\n",
    "        item['movie_writer'] = ';'.join(movie_writer).strip().replace(',',';').replace('\\'','\\\\\\'').replace('\\\"','\\\\\\\"').replace(':',';')\n",
    "        item['movie_roles'] = ';'.join(movie_roles).strip().replace(',',';').replace('\\'','\\\\\\'').replace('\\\"','\\\\\\\"').replace(':',';')\n",
    "        movie_detail_str = ''.join(movie_detail).strip()\n",
    "        movie_language_str = \".*语言:</span> (.+?)<br><span.*\".decode(\"utf8\")\n",
    "        movie_date_str = \".*上映日期:</span> <span property=\\\"v:initialReleaseDate\\\" content=\\\"(\\S+?)\\\">(\\S+?)</span>.*\".decode(\"utf8\")\n",
    "        movie_long_str = \".*片长:</span> <span property=\\\"v:runtime\\\" content=\\\"(\\d+).*\".decode(\"utf8\")\n",
    "        pattern_language =re.compile(movie_language_str,re.S)\n",
    "        pattern_date = re.compile(movie_date_str,re.S)\n",
    "        pattern_long = re.compile(movie_long_str,re.S)\n",
    "        movie_language = re.search(pattern_language,movie_detail_str)\n",
    "        movie_date = re.search(pattern_date,movie_detail_str)\n",
    "        movie_long = re.search(pattern_long,movie_detail_str)\n",
    "        item['movie_language'] = \"\"\n",
    "        if movie_language:\n",
    "            item['movie_language'] = movie_language.group(1).strip().replace(',',';').replace('\\'','\\\\\\'').replace('\\\"','\\\\\\\"').replace(':',';')\n",
    "        item['movie_date'] = \"\" \n",
    "        if movie_date:\n",
    "            item['movie_date'] = movie_date.group(1).strip().replace(',',';').replace('\\'','\\\\\\'').replace('\\\"','\\\\\\\"').replace(':',';')\n",
    "    \n",
    "    \n",
    "        item['movie_long'] = \"\"\n",
    "        if movie_long:\n",
    "            item['movie_long'] = movie_long.group(1)\n",
    "            #group()  strip()不知道是什么鬼\n",
    "        yield item\n",
    "            \n",
    "            \n",
    "#今天的尝试 再次失败=。=            \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-940b88e8f8d6>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-940b88e8f8d6>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    scrapy startproject myspider\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "scrapy startproject myspider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 6, 7, 42, 2, 31, 1]\n"
     ]
    }
   ],
   "source": [
    "def quc_sort(l,i,j):\n",
    "    if i>j:\n",
    "        return l\n",
    "    key = l[i]\n",
    "    while i < j:\n",
    "        while i < j and l[j] >= key:\n",
    "            j = j- 1\n",
    "        l[j]=l[j]\n",
    "        while i < j and l[j] <= key:\n",
    "            i = i +1\n",
    "        l[i] =l[j]\n",
    "    print l\n",
    "quc_sort([1,2,3,6,7,42,2,31,1],0,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 2, 2, 3, 6, 42, 31, 7]\n",
      "[1, 1, 2, 2, 3, 6, 7, 31, 42]\n",
      "[1, 1, 2, 2, 3, 6, 7, 31, 42]\n",
      "[1, 1, 2, 2, 3, 6, 7, 31, 42]\n",
      "[1, 1, 2, 2, 3, 6, 7, 31, 42]\n",
      "[1, 1, 2, 2, 3, 6, 7, 31, 42]\n"
     ]
    }
   ],
   "source": [
    "def quickSort(L, low, high):\n",
    "    i = low \n",
    "    j = high\n",
    "    if i >= j:\n",
    "        return L\n",
    "    key = L[i]\n",
    "    while i < j:\n",
    "        while i < j and L[j] >= key:\n",
    "            j = j-1                                                             \n",
    "        L[i] = L[j]\n",
    "        while i < j and L[i] <= key:    \n",
    "            i = i+1 \n",
    "        L[j] = L[i]\n",
    "    L[i] = key \n",
    "    quickSort(L, low, i-1)\n",
    "    quickSort(L, j+1, high)\n",
    "    print L\n",
    "quickSort([1,2,3,6,7,42,2,31,1],0,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 2, 2, 3, 6, 7, 31, 42]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([1,2,3,6,7,42,2,31,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datetime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-341f38ff27ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mexpression\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'datetime' is not defined"
     ]
    }
   ],
   "source": [
    "[expression for item in dir(datetime) if 1==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PYTHON', 'SCALA', 'MATLAB', 'SAS', 'SPSS']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst =['r','python','scala','matlab','sas','spss'] \n",
    "[i.upper() for  i in lst if len(i) >= 3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib2\n",
    "import requests\n",
    "import re\n",
    "from lxml import etree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "downloading  http://news.163.com/rank/\n",
      "downloading  http://news.163.com/special/0001386F/rank_whole.html\n",
      "downloading  http://news.163.com/special/0001386F/rank_news.html\n",
      "downloading  http://news.163.com/special/0001386F/rank_ent.html\n",
      "downloading  http://news.163.com/special/0001386F/rank_sports.html\n",
      "downloading  http://money.163.com/special/002526BH/rank.html\n",
      "downloading  http://news.163.com/special/0001386F/rank_tech.html\n",
      "downloading  http://news.163.com/special/0001386F/rank_auto.html\n",
      "downloading  http://news.163.com/special/0001386F/rank_lady.html\n",
      "downloading  http://news.163.com/special/0001386F/rank_house.html\n",
      "downloading  http://news.163.com/special/0001386F/rank_book.html\n",
      "downloading  http://news.163.com/special/0001386F/game_rank.html\n",
      "downloading  http://news.163.com/special/0001386F/rank_travel.html\n",
      "downloading  http://news.163.com/special/0001386F/rank_edu.html\n",
      "downloading  http://news.163.com/special/0001386F/rank_gongyi.html\n",
      "downloading  http://news.163.com/special/0001386F/rank_campus.html\n",
      "downloading  http://news.163.com/special/0001386F/rank_media.html\n",
      "downloading  http://news.163.com/special/0001386F/rank_video.html\n",
      "downloading  http://news.163.com/special/rank_m/\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "def StringListSave(save_path, filename,slist):\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    path = save_path+'/'+filename + '.txt'\n",
    "    with open(path,'w+') as fp:\n",
    "        for s in slist:\n",
    "            fp.write('%s\\t\\t%s\\n' % (s[0].encode('utf8'),s[1].encode('utf8')))\n",
    "def Page_Info(myPage):\n",
    "    mypage_Info = re.findall(r'<div class=\"titleBar\" id=\".*?\"><h2>(.*?)</h2><div class=\"more\"><a href=\"(.*?)\">.*?</a></div></div>', myPage, re.S)\n",
    "    return mypage_Info\n",
    "#just return  a list\n",
    "\n",
    "def New_Page_Info(new_page):\n",
    "    '''Regex(slowly) or Xpath(fast)'''\n",
    "    # new_page_Info = re.findall(r'<td class=\".*?\">.*?<a href=\"(.*?)\\.html\".*?>(.*?)</a></td>', new_page, re.S)\n",
    "    # # new_page_Info = re.findall(r'<td class=\".*?\">.*?<a href=\"(.*?)\">(.*?)</a></td>', new_page, re.S) # bugs\n",
    "    # results = []\n",
    "    # for url, item in new_page_Info:\n",
    "    #     results.append((item, url+\".html\"))\n",
    "    # return results\n",
    "    dom = etree.HTML(new_page)\n",
    "    new_items = dom.xpath('//tr/td/a/text()')\n",
    "    new_urls = dom.xpath('//tr/td/a/@href')\n",
    "    assert(len(new_items) == len(new_urls))\n",
    "    return zip(new_items, new_urls)\n",
    "\n",
    "def Spider(url):\n",
    "    i = 0\n",
    "    print \"downloading \", url\n",
    "    myPage = requests.get(url).content.decode(\"gbk\")\n",
    "    # myPage = urllib2.urlopen(url).read().decode(\"gbk\")\n",
    "    myPageResults = Page_Info(myPage)\n",
    "    save_path = u\"网易新闻抓取\"\n",
    "    filename = str(i)+\"_\"+u\"新闻排行榜\"\n",
    "    StringListSave(save_path, filename, myPageResults)\n",
    "    i += 1\n",
    "    for item, url in myPageResults:\n",
    "        print \"downloading \", url\n",
    "        new_page = requests.get(url).content.decode(\"gbk\")\n",
    "        # new_page = urllib2.urlopen(url).read().decode(\"gbk\")\n",
    "        newPageResults = New_Page_Info(new_page)\n",
    "        filename = str(i)+\"_\"+item\n",
    "        StringListSave(save_path, filename, newPageResults)\n",
    "        i += 1\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print \"start\"\n",
    "    start_url = \"http://news.163.com/rank/\"\n",
    "    Spider(start_url)\n",
    "    print \"end\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import itchat\n",
    "\n",
    "KEY = '8edce3ce905a4c1dbb965e6b35c3834d'\n",
    "\n",
    "def get_response(msg):\n",
    "    # 这里我们就像在“3. 实现最简单的与图灵机器人的交互”中做的一样\n",
    "    # 构造了要发送给服务器的数据\n",
    "    apiUrl = 'http://www.tuling123.com/openapi/api'\n",
    "    data = {\n",
    "        'key'    : KEY,\n",
    "        'info'   : msg,\n",
    "        'userid' : 'wechat-robot',\n",
    "    }\n",
    "    try:\n",
    "        r = requests.post(apiUrl, data=data).json()\n",
    "        # 字典的get方法在字典没有'text'值的时候会返回None而不会抛出异常\n",
    "        return r.get('text')\n",
    "    # 为了防止服务器没有正常响应导致程序异常退出，这里用try-except捕获了异常\n",
    "    # 如果服务器没能正常交互（返回非json或无法连接），那么就会进入下面的return\n",
    "    except:\n",
    "        # 将会返回一个None\n",
    "        return\n",
    "    # 这里是我们在“1. 实现微信消息的获取”中已经用到过的同样的注册方法\n",
    "@itchat.msg_register(itchat.content.TEXT)\n",
    "def tuling_reply(msg):\n",
    "    # 为了保证在图灵Key出现问题的时候仍旧可以回复，这里设置一个默认回复\n",
    "    defaultReply = 'I received: ' + msg['Text']\n",
    "    # 如果图灵Key出现问题，那么reply将会是None\n",
    "    reply = get_response(msg['Text'])\n",
    "    # a or b的意思是，如果a有内容，那么返回a，否则返回b\n",
    "     # 有内容一般就是指非空或者非None，你可以用`if a: print('True')`来测试\n",
    "    return reply or defaultReply\n",
    "\n",
    "# 为了让实验过程更加方便（修改程序不用多次扫码），我们使用热启动\n",
    "itchat.auto_login(hotReload=True)\n",
    "itchat.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named seaborn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-90907e654a18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatlotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named seaborn"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import matlotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "listing_data = pd.read_csv('./boston-airbnb-open-data/listings.csv',usecols=['property_type','room_type','accommodates',\n",
    "                                                                             'bedrooms','price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "listing_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "price_date_list = listing_data['price'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "price_list = [float(item.replace(',','')[1:])for item in price_data_list] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_data['price'] = price_list\n",
    "listing_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figprure(figsize = (12, 6))\n",
    "sns.boxplot(x = 'property_type',y = 'price',data = listing_data)\n",
    "xt = plt.xticks(rotation = 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.factorplot('accommodates','price',data = listing_data,color = 'm',\\\n",
    "              estimator= np.nedian, size = 4.5 ,aspect = 1.35)\n",
    "xt = plt.xticks(rotation = 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "sns.heatmap(listing_data.groupby(['property_type','bedroom'])['price'].mean().unstack(),annot = True,fmt = '.0f')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
